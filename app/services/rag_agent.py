# RAG èˆ‡ AI Agent å°è£ (æ–°ç‰ˆ LangChain æ¶æ§‹ï¼Œä½¿ç”¨ HuggingFace InferenceClient)

import os
import time
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings

# âœ… æ–°å¢ï¼šç›´æ¥ä½¿ç”¨ huggingface_hub.InferenceClient
from huggingface_hub import InferenceClient
from langchain_core.language_models import LLM
from typing import Optional, List
from pydantic import Field

# âœ… å»ºç«‹ä¸€å€‹ç°¡å–®çš„ LangChain LLM wrapperï¼Œå°è£ InferenceClient chat.completions
class HuggingFaceLLM(LLM):
    temperature: float = Field(default=0.0)
    max_tokens: int = Field(default=512)
    client: Optional[InferenceClient] = Field(default=None, exclude=True)
    model: str = Field(default="Qwen/Qwen2-7B-Instruct")

    def __init__(self, model: str, token: Optional[str] = None, provider: str = "auto", **kwargs):
        super().__init__(**kwargs)
        object.__setattr__(self, "client", InferenceClient(provider=provider, api_key=token))
        object.__setattr__(self, "model", model)

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        try:
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature,
            )
            return resp.choices[0].message.content
        except Exception as e:
            return f"[Error] HuggingFace LLM å‘¼å«å¤±æ•—: {repr(e)}"

    @property
    def _llm_type(self) -> str:
        return "huggingface_chat_completions"

class RAGAgent:
    def __init__(self, name: str, index_path: str, max_retries: int = 3, retry_delay: int = 5, debug_prompt: bool = True):
        load_dotenv()
        self.name = name
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.debug_prompt = debug_prompt

        # Embeddings
        engine = os.getenv("EMBEDDING_ENGINE", "huggingface").lower()
        if engine == "openai":
            if not self.api_key:
                raise ValueError("âŒ æœªæ‰¾åˆ° OPENAI_API_KEYï¼Œè«‹ç¢ºèª .env æª”æ¡ˆæ˜¯å¦æ­£ç¢ºè¨­å®š")
            print(f"âœ… ä½¿ç”¨ OpenAI Embeddings for agent: {self.name}")
            self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        elif engine == "huggingface":
            print(f"âœ… ä½¿ç”¨ HuggingFace Embeddings (all-MiniLM-L6-v2) for agent: {self.name}")
            self.embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        else:
            raise ValueError(f"âŒ æœªçŸ¥çš„ EMBEDDING_ENGINE: {engine}")

        print(f"âœ… å·²è¼‰å…¥ OpenAI API é‡‘é‘°ï¼š{self.api_key[:5]}...ï¼ˆå·²é®è”½ï¼‰")

        # Vector DB
        self.vector_db = FAISS.load_local(
            index_path,
            self.embeddings,
            allow_dangerous_deserialization=True
        )
        self.retriever = self.vector_db.as_retriever()  # âœ… ä¿å­˜ retriever

        # LLM
        llm_engine = os.getenv("LLM_ENGINE", "huggingface").lower()
        if llm_engine == "openai":
            if not self.api_key:
                raise ValueError("âŒ æœªæ‰¾åˆ° OPENAI_API_KEYï¼Œè«‹ç¢ºèª .env æª”æ¡ˆæ˜¯å¦æ­£ç¢ºè¨­å®š")
            print(f"âœ… ä½¿ç”¨ OpenAI LLM for agent: {self.name}")
            self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        elif llm_engine == "huggingface":
            print(f"âœ… ä½¿ç”¨ HuggingFace InferenceClient ChatCompletions for agent: {self.name}")
            repo_id = os.getenv("HF_REPO_ID", "Qwen/Qwen2-7B-Instruct")
            self.llm = HuggingFaceLLM(
                model=repo_id,
                token=os.getenv("HUGGINGFACEHUB_API_TOKEN"),
                provider="featherless-ai",
                temperature=0,
                max_tokens=512
            )
        else:
            raise ValueError(f"âŒ æœªçŸ¥çš„ LLM_ENGINE: {llm_engine}")

        # Prompt â†’ âœ… ä¿å­˜ç‚ºæˆå“¡
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹çŸ¥è­˜å‹åŠ©ç†ï¼Œè«‹æ ¹æ“šæª¢ç´¢åˆ°çš„æ–‡ä»¶å›ç­”å•é¡Œã€‚"),
            ("human", "{input}\n\næª¢ç´¢åˆ°çš„å…§å®¹ï¼š{context}")
        ])

        # Docs chain â†’ âœ… ä¿å­˜ç‚ºæˆå“¡
        self.docs_chain = create_stuff_documents_chain(self.llm, self.prompt)

        # QA chain
        self.qa_chain = create_retrieval_chain(self.retriever, self.docs_chain)

    def answer_question(self, question: str) -> str:
        import traceback
        for attempt in range(1, self.max_retries + 1):
            try:
                print(f"ğŸ” å˜—è©¦ç¬¬ {attempt} æ¬¡è™•ç†å•é¡Œï¼š{question}")

                # å…ˆç”¨ retriever å–å¾—ç›¸é—œæ–‡ä»¶ä¸¦çµ„æˆ context
                docs = self.retriever.get_relevant_documents(question)
                context_text = "\n\n".join([d.page_content for d in docs])

                # âœ… åªæœ‰åœ¨ debug_prompt=True æ™‚æ‰å°å‡ºå®Œæ•´ Prompt
                if self.debug_prompt:
                    formatted_prompt = self.prompt.format(
                        input=question,
                        context=context_text
                    )
                    print("ğŸ“ AI çœ‹åˆ°çš„ Prompt:\n", formatted_prompt)

                # åŸ·è¡Œ QA chain
                result = self.qa_chain.invoke({"input": question})
                return result.get("answer") or str(result)

            except Exception as e:
                print(f"âš ï¸ å˜—è©¦ç¬¬ {attempt} æ¬¡å¤±æ•—ï¼ŒéŒ¯èª¤å¦‚ä¸‹ï¼š")
                print("Exception repr:", repr(e))
                traceback.print_exc()
                if hasattr(e, "errors"):
                    print("Pydantic errors:", e.errors())
                time.sleep(self.retry_delay)
        print("âŒ æ‰€æœ‰é‡è©¦çš†å¤±æ•—")
        return None